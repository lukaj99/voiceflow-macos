import Foundation
@preconcurrency import AVFoundation
import AsyncAlgorithms

/// Modern audio manager using Swift 6 actor isolation for safe audio processing
/// Separates UI state (@MainActor) from audio processing (dedicated actor)
@MainActor
public class AudioManager: ObservableObject {
    
    // MARK: - Published Properties
    @Published public var isRecording = false
    @Published public var audioLevel: Float = 0.0
    
    // MARK: - Private Properties
    private let audioProcessor = AudioProcessingActor()
    
    // MARK: - Delegate
    public weak var delegate: (any AudioManagerDelegate)?
    
    // MARK: - Initialization
    public init() {
        print("üé§ Modern AudioManager initialized with actor isolation")
        setupAudioStreaming()
    }
    
    // MARK: - Private Methods
    
    /// Setup audio level streaming from the processor
    private func setupAudioStreaming() {
        // Stream audio levels from the processor to UI in background task
        Task { [weak self] in
            guard let self = self else { return }
            for await level in audioProcessor.audioLevelStream {
                await MainActor.run { [weak self] in
                    self?.audioLevel = level
                }
            }
        }
    }
    
    // MARK: - Public Methods
    
    public func startRecording() async throws {
        print("üé§ Starting audio recording...")
        
        guard !isRecording else { 
            print("‚ö†Ô∏è Already recording")
            return 
        }
        
        // Start recording via the audio processor
        try await audioProcessor.startRecording { [weak self] audioData in
            Task { @MainActor [weak self] in
                self?.delegate?.audioManager(self!, didReceiveAudioData: audioData)
            }
        }
        
        isRecording = true
        print("‚úÖ Audio recording started")
    }
    
    public func stopRecording() {
        print("üé§ Stopping audio recording...")
        
        guard isRecording else { return }
        
        Task {
            await audioProcessor.stopRecording()
        }
        
        isRecording = false
        audioLevel = 0.0
        
        print("‚úÖ Audio recording stopped")
    }
    
    public func pauseRecording() {
        print("üé§ Pausing audio recording...")
        
        guard isRecording else { return }
        
        Task {
            await audioProcessor.pauseRecording()
        }
        
        print("‚è∏Ô∏è Audio recording paused")
    }
    
    public func resumeRecording() {
        print("üé§ Resuming audio recording...")
        
        Task {
            await audioProcessor.resumeRecording()
        }
        
        print("‚ñ∂Ô∏è Audio recording resumed")
    }
    
}

// MARK: - Audio Processing Actor

/// Dedicated actor for audio processing, isolated from UI updates
actor AudioProcessingActor {
    
    // MARK: - Properties
    
    private let audioEngine = AVAudioEngine()
    private nonisolated(unsafe) var audioFormat: AVAudioFormat?
    private nonisolated(unsafe) var targetFormat: AVAudioFormat?
    private nonisolated(unsafe) var audioConverter: AVAudioConverter?
    private var isRecording = false
    
    // Audio level streaming
    private let audioLevelContinuation: AsyncStream<Float>.Continuation
    public let audioLevelStream: AsyncStream<Float>
    
    // Audio data callback
    private var audioDataCallback: ((Data) -> Void)?
    
    // MARK: - Initialization
    
    init() {
        (audioLevelStream, audioLevelContinuation) = AsyncStream.makeStream(of: Float.self)
        print("üé§ AudioProcessingActor initialized")
        
        // Audio formats will be setup when starting recording
    }
    
    // MARK: - Public Interface
    
    func startRecording(audioDataCallback: @escaping @Sendable (Data) -> Void) async throws {
        guard !isRecording else {
            print("‚ö†Ô∏è Audio processor already recording")
            return
        }
        
        self.audioDataCallback = audioDataCallback
        
        // Setup audio formats
        await setupAudioFormats()
        
        // Request microphone permission
        guard await requestMicrophonePermission() else {
            throw AudioError.microphonePermissionDenied
        }
        
        try setupAudioEngine()
        try audioEngine.start()
        isRecording = true
        
        print("üé§ AudioProcessingActor started recording")
    }
    
    func stopRecording() async {
        guard isRecording else { return }
        
        audioEngine.inputNode.removeTap(onBus: 0)
        audioEngine.stop()
        isRecording = false
        audioDataCallback = nil
        
        // Send zero audio level
        audioLevelContinuation.yield(0.0)
        
        print("üé§ AudioProcessingActor stopped recording")
    }
    
    func pauseRecording() async {
        guard isRecording else { return }
        
        audioEngine.pause()
        print("üé§ AudioProcessingActor paused recording")
    }
    
    func resumeRecording() async {
        guard isRecording else { return }
        
        try? audioEngine.start()
        print("üé§ AudioProcessingActor resumed recording")
    }
    
    // MARK: - Private Methods
    
    private func setupAudioFormats() async {
        let inputNode = audioEngine.inputNode
        
        // Input format from microphone
        audioFormat = inputNode.outputFormat(forBus: 0)
        
        // Target format for Deepgram: 16kHz, 16-bit PCM, mono
        targetFormat = AVAudioFormat(
            commonFormat: .pcmFormatInt16,
            sampleRate: 16000,
            channels: 1,
            interleaved: false
        )
        
        // Create audio converter
        if let audioFormat = audioFormat, let targetFormat = targetFormat {
            audioConverter = AVAudioConverter(from: audioFormat, to: targetFormat)
            print("üîß Audio converter created: \(audioFormat.sampleRate)Hz ‚Üí \(targetFormat.sampleRate)Hz")
        }
    }
    
    private func requestMicrophonePermission() async -> Bool {
        // macOS microphone permission is handled at system level
        print("üé§ Microphone permission assumed (macOS)")
        return true
    }
    
    private func setupAudioEngine() throws {
        guard let audioFormat = audioFormat else {
            throw AudioError.audioFormatNotSupported
        }
        
        let inputNode = audioEngine.inputNode
        let bufferSize: AVAudioFrameCount = 1024
        
        inputNode.installTap(onBus: 0, bufferSize: bufferSize, format: audioFormat) { [weak self] buffer, _ in
            // Process buffer synchronously on real-time audio thread
            guard let self = self else { return }
            
            // Calculate audio level synchronously
            let level = self.calculateAudioLevel(from: buffer)
            
            // Convert audio synchronously
            guard let convertedBuffer = self.convertAudioFormat(buffer),
                  let audioData = self.extractPCMData(from: convertedBuffer) else {
                return
            }
            
            // Send processed data to actor asynchronously
            Task { [weak self] in
                await self?.handleProcessedAudio(level: level, data: audioData)
            }
        }
        
        print("üîß Audio tap installed with buffer size: \(bufferSize)")
    }
    
    /// Handle processed audio data from the real-time thread
    private func handleProcessedAudio(level: Float, data: Data) async {
        // Stream audio level
        audioLevelContinuation.yield(level)
        
        // Debug: Log audio data being sent
        if data.count > 0 {
            print("üé§ Sending \(data.count) bytes of audio data (level: \(String(format: "%.2f", level)))")
        }
        
        // Send audio data via callback
        audioDataCallback?(data)
    }
    
    nonisolated private func calculateAudioLevel(from buffer: AVAudioPCMBuffer) -> Float {
        guard let channelData = buffer.floatChannelData?[0] else { return 0.0 }
        
        let channelDataArray = Array(UnsafeBufferPointer(start: channelData, count: Int(buffer.frameLength)))
        let rms = sqrt(channelDataArray.map { $0 * $0 }.reduce(0, +) / Float(channelDataArray.count))
        let decibels = 20 * log10(rms)
        
        // Normalize to 0-1 range (assuming -60dB to 0dB range)
        return max(0, min(1, (decibels + 60) / 60))
    }
    
    nonisolated private func convertAudioFormat(_ buffer: AVAudioPCMBuffer) -> AVAudioPCMBuffer? {
        // Access to these properties is safe because they're set once during initialization
        // and never modified during recording
        guard let converter = self.audioConverter,
              let targetFormat = self.targetFormat else { return nil }
        
        let targetCapacity = AVAudioFrameCount(Double(buffer.frameLength) * targetFormat.sampleRate / buffer.format.sampleRate)
        guard let convertedBuffer = AVAudioPCMBuffer(pcmFormat: targetFormat, frameCapacity: targetCapacity) else {
            return nil
        }
        
        var error: NSError?
        let status = converter.convert(to: convertedBuffer, error: &error) { _, outStatus in
            outStatus.pointee = .haveData
            return buffer
        }
        
        if status == .error {
            print("‚ùå Audio conversion error: \(error?.localizedDescription ?? "Unknown")")
            return nil
        }
        
        return convertedBuffer
    }
    
    nonisolated private func extractPCMData(from buffer: AVAudioPCMBuffer) -> Data? {
        guard let channelData = buffer.int16ChannelData?[0] else { return nil }
        
        let byteCount = Int(buffer.frameLength) * MemoryLayout<Int16>.size
        return Data(bytes: channelData, count: byteCount)
    }
    
    deinit {
        audioLevelContinuation.finish()
    }
}

// MARK: - Delegate Protocol

public protocol AudioManagerDelegate: AnyObject {
    func audioManager(_ manager: AudioManager, didReceiveAudioData data: Data)
}

// MARK: - Audio Errors

public enum AudioError: Error, LocalizedError {
    case microphonePermissionDenied
    case audioFormatNotSupported
    case audioEngineStartFailed
    
    public var errorDescription: String? {
        switch self {
        case .microphonePermissionDenied:
            return "Microphone permission is required"
        case .audioFormatNotSupported:
            return "Audio format not supported"
        case .audioEngineStartFailed:
            return "Failed to start audio engine"
        }
    }
}